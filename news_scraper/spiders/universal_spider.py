import scrapy
from scrapy.loader import ItemLoader
from datetime import datetime, timedelta
from typing import Optional


# å°è¯•å¯¼å…¥é¡¹ç›®æ¨¡å—
try:
    from news_scraper.items import NewsItem, NewsItemLoader, generate_news_id
    from news_scraper.utils.extractor import MultiSiteExtractor
except ImportError:
    # å¦‚æœåœ¨å¼€å‘ç¯å¢ƒï¼Œæ·»åŠ è·¯å¾„
    import sys
    from pathlib import Path

    project_root = Path(__file__).parent.parent.parent
    sys.path.insert(0, str(project_root))
    from news_scraper.items import NewsItem, NewsItemLoader
    from news_scraper.utils.extractor import MultiSiteExtractor


class UniversalNewsSpider(scrapy.Spider):
    """
    é€šç”¨æ–°é—»çˆ¬è™«
    é€šè¿‡é…ç½®æ–‡ä»¶æ”¯æŒå¤šä¸ªæ–°é—»ç½‘ç«™ï¼Œæ— éœ€ä¸ºæ¯ä¸ªç½‘ç«™å†™ç‹¬ç«‹çˆ¬è™«

    ä½¿ç”¨ç¤ºä¾‹:
        # çˆ¬å–æ‰€æœ‰å¯ç”¨çš„æ–°é—»æº
        scrapy crawl universal_news

        # çˆ¬å–æŒ‡å®šæ–°é—»æº
        scrapy crawl universal_news -a sources=cnn,bbc

        # çˆ¬å–è¿‘7å¤©çš„æ–°é—»
        scrapy crawl universal_news -a days_back=7

        # ç»„åˆä½¿ç”¨
        scrapy crawl universal_news -a sources=cnn -a days_back=3
    """

    name = "universal_news"

    # è‡ªå®šä¹‰è®¾ç½®
    custom_settings = {
        "CONCURRENT_REQUESTS_PER_DOMAIN": 2,
        "DOWNLOAD_DELAY": 2,
        "RANDOMIZE_DOWNLOAD_DELAY": True,
        "ROBOTSTXT_OBEY": False,
    }

    def __init__(
        self,
        sources: Optional[str] = None,
        days_back: int = 1,
        config_path: str = "config/news_sources.json",
        *args,
        **kwargs,
    ):
        """
        åˆå§‹åŒ–çˆ¬è™«

        Args:
            sources: æŒ‡å®šè¦çˆ¬å–çš„æ–°é—»æºï¼Œé€—å·åˆ†éš”ï¼Œå¦‚ 'cnn,bbc'
                    å¦‚æœä¸ºNoneåˆ™çˆ¬å–æ‰€æœ‰å¯ç”¨çš„æº
            days_back: çˆ¬å–å¤šå°‘å¤©å†…çš„æ–°é—»ï¼Œé»˜è®¤1å¤©
            config_path: é…ç½®æ–‡ä»¶è·¯å¾„
        """
        super().__init__(*args, **kwargs)

        # åŠ è½½å­—æ®µæå–å™¨
        self.multi_extractor = MultiSiteExtractor(config_path)

        # ç¡®å®šè¦é‡‡é›†åˆ°æ–°é—»æº
        if sources:
            self.target_sources = [s.strip() for s in sources.split(",")]
            available = self.multi_extractor.get_all_sources()
            invalid = [s for s in self.target_sources if s not in available]
            if invalid:
                self.logger.warning(f'ä»¥ä¸‹æ–°é—»æºä¸å­˜åœ¨æˆ–æœªå¯ç”¨: {", ".join(invalid)}')
            self.target_sources = [s for s in self.target_sources if s in available]
        else:
            self.target_sources = self.multi_extractor.get_all_sources()

        if not self.target_sources:
            raise ValueError("æ²¡æœ‰å¯ç”¨çš„æ–°é—»æºï¼è¯·æ£€æŸ¥é…ç½®æ–‡ä»¶æˆ–sourceså‚æ•°")

        self.days_back = int(days_back)
        self.start_date = datetime.now() - timedelta(days=self.days_back)

        # è¾“å‡ºå¯åŠ¨ä¿¡æ¯
        self.logger.info("=" * 60)
        self.logger.info(f"é€šç”¨æ–°é—»çˆ¬è™«å¯åŠ¨")
        self.logger.info(
            f'ç›®æ ‡æ–°é—»æº ({len(self.target_sources)}): {", ".join(self.target_sources)}'
        )
        self.logger.info(
            f'æ—¶é—´èŒƒå›´: æœ€è¿‘ {self.days_back} å¤© (ä» {self.start_date.strftime("%Y-%m-%d")} èµ·)'
        )
        self.logger.info("=" * 60)

        # åŠ¨æ€è®¾ç½®allowed_domainså’Œstart_urls
        self._setup_urls()

        # ç»Ÿè®¡ä¿¡æ¯
        self.stats = {
            "pages_crawled": 0,
            "articles_found": 0,
            "articles_scraped": 0,
            "articles_failed": 0,
        }

    def start_requests(self):
        """
        ç”Ÿæˆåˆå§‹è¯·æ±‚
        """
        for url in self.start_urls:
            # è¯†åˆ«æ–°é—»æº
            source_id = self._identify_source(url)

            yield scrapy.Request(
                url=url,
                callback=self.parse,
                errback=self.handle_error,
                meta={"source_id": source_id, "page_type": "list"},
                dont_filter=True,
            )

    def parse(self, response):
        """
        è§£æåˆ—è¡¨é¡µï¼Œæå–æ–‡ç« é“¾æ¥

        Args:
            response: Scrapy Responseå¯¹è±¡
        """
        self.stats["pages_crawled"] += 1

        # ä»metaä¸­è·å–æ–°é—»æºID
        source_id = response.meta.get("source_id")
        if not source_id:
            source_id = self._identify_source(response.url)
        if not source_id:
            self.logger.warning(f"âŒ æ— æ³•è¯†åˆ«æ–°é—»æº: {response.url}")
            return

        extractor = self.multi_extractor.get_extractor(source_id)
        config = self.multi_extractor.get_config(source_id)

        if not extractor or not config:
            self.logger.error(f"âŒ æ‰¾ä¸åˆ°æ–°é—»æºé…ç½®: {source_id}")
            return

        self.logger.info(f'ğŸ“„ è§£æåˆ—è¡¨é¡µ: {response.url} ({config.get("name")})')

        # æå–æ–‡ç« é“¾æ¥
        links_config = config.get("selectors", {}).get("article_links", {})
        article_links = extractor.extract_field(response, "article_links", links_config)
        if not article_links:
            self.logger.warning(f"âš  æœªæå–åˆ°æ–‡ç« é“¾æ¥: {response.url}")
            return
        if isinstance(article_links, str):
            article_links = [article_links]

        self.logger.info(f"ğŸ“° æ‰¾åˆ° {len(article_links)} ä¸ªæ–‡ç« é“¾æ¥")
        self.stats["articles_found"] += len(article_links)

        # éå†é“¾æ¥
        for link in article_links:
            # æ„å»ºå®Œæ•´URL
            article_url = response.urljoin(link)

            # éªŒè¯æ˜¯å¦ä¸ºæœ‰æ•ˆæ–‡ç« URL
            if not extractor.is_valid_article_url(article_url):
                continue

            # å‘èµ·è¯¦æƒ…é¡µè¯·æ±‚
            yield scrapy.Request(
                url=article_url,
                callback=self.parse_article,
                errback=self.handle_error,
                meta={
                    "source_id": source_id,
                    "source_config": config,
                    "page_type": "article",
                },
                dont_filter=False,
            )

    def parse_article(self, response):
        """
        è§£ææ–‡ç« è¯¦æƒ…é¡µ
        ä½¿ç”¨é…ç½®åŒ–çš„æå–å™¨è‡ªåŠ¨æå–æ‰€æœ‰å­—æ®µ

        Args:
            response: Scrapy Responseå¯¹è±¡
        """
        # ä»metaä¸­è·å–æ–°é—»æºIDå’Œé…ç½®
        source_id = response.meta["source_id"]
        source_config = response.meta["source_config"]

        try:
            # æŸ¥çœ‹æ˜¯å¦æœ‰å¯¹åº”çš„å­—æ®µæå–å™¨
            extractor = self.multi_extractor.get_extractor(source_id)
            if not extractor:
                self.logger.error(f"âŒ æ‰¾ä¸åˆ°æå–å™¨: {source_id}")
                return

            self.logger.info(f"ğŸ“– è§£ææ–‡ç« : {response.url}")

            # ä½¿ç”¨æå–å™¨æå–æ‰€æœ‰å­—æ®µ
            extracted_data = extractor.extract_all_fields(response)

            # æ£€æŸ¥å¿…å¡«å­—æ®µ
            if not extracted_data.get("title"):
                self.logger.error(f"âŒ æ ‡é¢˜æå–å¤±è´¥ï¼Œè·³è¿‡: {response.url}")
                self.stats["articles_failed"] += 1
                return

            if not extracted_data.get("content"):
                self.logger.warning(f"âš  å†…å®¹æå–å¤±è´¥ï¼Œä½†ç»§ç»­å¤„ç†: {response.url}")

            # æ„å»ºItem
            loader = NewsItemLoader(item=NewsItem(), response=response)
            loader.add_value("news_id", generate_news_id(response.url))
            loader.add_value("url", response.url)
            loader.add_value("crawl_time", datetime.now().isoformat())
            loader.add_value("source_name", source_config.get("name"))
            loader.add_value("source_country", source_config.get("country"))
            loader.add_value("language", source_config.get("language"))
            category = self._extract_category(response.url, source_config)
            if category:
                loader.add_value("category", category)

            # æ·»åŠ æå–åˆ°çš„å­—æ®µ
            allowed_fields = set(NewsItem.fields.keys())
            excluded_fields = {"article_links", "list_page_url"}
            for field_name, field_value in extracted_data.items():
                # è·³è¿‡æ’é™¤çš„å­—æ®µ
                if field_name in excluded_fields:
                    continue
                if field_name in allowed_fields:
                    loader.add_value(field_name, field_value)

            # æ£€æŸ¥æ—¥æœŸæœ‰æ•ˆæ€§
            publish_time = extracted_data.get("publish_time")
            if publish_time and not self._is_valid_date(publish_time):
                self.logger.info(f"â° æ–‡ç« è¿‡æ—§ï¼Œè·³è¿‡: {response.url}")
                return

            item = loader.load_item()
            self.stats["articles_scraped"] += 1
            self.logger.info(f'âœ… æˆåŠŸæå–: {item.get("title", "")[:50]}...')

            yield item
        except KeyError as e:
            self.logger.error(f"âŒ å­—æ®µé”™è¯¯ {str(e)} - {response.url}")
            self.stats["articles_failed"] += 1
        except Exception as e:
            self.logger.error(f"âŒ {e.__class__.__name__}: {str(e)[:50]}")
            self.stats["articles_failed"] += 1

    def handle_error(self, failure):
        """
        ç»Ÿä¸€é”™è¯¯å¤„ç†

        Args:
            failure: Twisted Failureå¯¹è±¡
        """
        request = failure.request
        self.logger.error(f"âŒ è¯·æ±‚å¤±è´¥: {request.url}")
        self.logger.error(f"   é”™è¯¯ç±»å‹: {failure.type.__name__}")
        self.logger.error(f"   é”™è¯¯ä¿¡æ¯: {failure.value}")

        if request.meta.get("page_type") == "article":
            self.stats["articles_failed"] += 1

    def closed(self, reason):
        """
        çˆ¬è™«å…³é—­æ—¶çš„å›è°ƒ
        è¾“å‡ºç»Ÿè®¡ä¿¡æ¯

        Args:
            reason: å…³é—­åŸå› 
        """
        self.logger.info("=" * 60)
        self.logger.info(f"çˆ¬è™«å…³é—­: {reason}")
        self.logger.info("ç»Ÿè®¡ä¿¡æ¯:")
        self.logger.info(f'  åˆ—è¡¨é¡µçˆ¬å–: {self.stats["pages_crawled"]} é¡µ')
        self.logger.info(f'  æ–‡ç« å‘ç°: {self.stats["articles_found"]} ç¯‡')
        self.logger.info(f'  æ–‡ç« æˆåŠŸ: {self.stats["articles_scraped"]} ç¯‡')
        self.logger.info(f'  æ–‡ç« å¤±è´¥: {self.stats["articles_failed"]} ç¯‡')
        if self.stats["articles_found"] > 0:
            success_rate = (
                self.stats["articles_scraped"] / self.stats["articles_found"]
            ) * 100
            self.logger.info(f"  æˆåŠŸç‡: {success_rate:.1f}%")
        self.logger.info("=" * 60)

    def _setup_urls(self):
        """
        æ ¹æ®é…ç½®åŠ¨æ€è®¾ç½®allowed_domainså’Œstart_urls
        """
        self.allowed_domains = []
        self.start_urls = []

        for source_id in self.target_sources:
            config = self.multi_extractor.get_config(source_id)
            if not config:
                continue

            # æ·»åŠ åŸŸå
            domain = config.get("domain")
            if domain and domain not in self.allowed_domains:
                self.allowed_domains.append(domain)

            # æ·»åŠ èµ·å§‹URL
            list_pages = config.get("list_pages", [])
            for page in list_pages:
                url = page.get("url")
                if url and url not in self.start_urls:
                    self.start_urls.append(url)

        self.logger.info(f"å·²é…ç½® {len(self.allowed_domains)} ä¸ªåŸŸå")
        self.logger.info(f"å·²é…ç½® {len(self.start_urls)} ä¸ªèµ·å§‹URL")

    def _identify_source(self, url: str) -> Optional[str]:
        """
        æ ¹æ®URLè¯†åˆ«æ–°é—»æº

        Args:
            url: ç½‘ç«™URL

        Returns:
            æ–°é—»æºIDï¼Œå¦‚æœæ— æ³•è¯†åˆ«è¿”å›None
        """
        for source_id in self.target_sources:
            config = self.multi_extractor.get_config(source_id)
            if not config:
                continue

            domain = config.get("domain")
            if domain and domain in url:
                return source_id

        return None

    def _extract_category(self, url: str, config: dict) -> Optional[str]:
        """
        ä»URLæˆ–é…ç½®ä¸­æå–åˆ†ç±»

        Args:
            url: æ–‡ç« URL
            config: æ–°é—»æºé…ç½®

        Returns:
            åˆ†ç±»åç§°
        """
        # ä»list_pagesé…ç½®ä¸­åŒ¹é…åˆ†ç±»
        list_pages = config.get("list_pages", [])
        for page in list_pages:
            page_url = page.get("url", "")
            category = page.get("category", "general")

            # ç®€å•åŒ¹é…ï¼šå¦‚æœæ–‡ç« URLåŒ…å«list_pageçš„è·¯å¾„
            if page_url:
                page_path = page_url.split("/")[-1]
                if page_path and page_path in url:
                    return category

        # é»˜è®¤åˆ†ç±»
        return "general"

    def _is_valid_date(self, date_string: str) -> bool:
        """
        æ£€æŸ¥æ—¥æœŸæ˜¯å¦åœ¨æœ‰æ•ˆèŒƒå›´å†…

        Args:
            date_string: ISOæ ¼å¼æ—¥æœŸå­—ç¬¦ä¸²

        Returns:
            æ˜¯å¦æœ‰æ•ˆ
        """
        try:
            publish_time = datetime.fromisoformat(date_string.replace("Z", "+00:00"))
            return publish_time >= self.start_date
        except:
            return True  # å¦‚æœè§£æå¤±è´¥ï¼Œé»˜è®¤é‡‡é›†
