# Define your item pipelines here
#
# Don't forget to add your pipeline to the ITEM_PIPELINES setting
# See: https://docs.scrapy.org/en/latest/topics/item-pipeline.html


import logging
from datetime import datetime
from itemadapter import ItemAdapter
from scrapy.exceptions import DropItem


class NewsScraperPipeline:
    def process_item(self, item, spider):
        return item


class ValidationPipeline:
    """
    æ•°æ®éªŒè¯ç®¡é“
    éªŒè¯å¿…å¡«å­—æ®µå’Œæ•°æ®æ ¼å¼
    """

    required_fields = ["title", "url", "source_name"]

    # æœ€å°å†…å®¹é•¿åº¦ï¼ˆå­—ç¬¦æ•°ï¼‰
    MIN_TITLE_LENGTH = 10
    MIN_CONTENT_LENGTH = 50

    def __init__(self):
        self.logger = logging.getLogger(self.__class__.__name__)

    def process_item(self, item, spider):
        """
        éªŒè¯æ•°æ®å®Œæ•´æ€§

        Args:
            item: NewsItemå¯¹è±¡
            spider: Spiderå¯¹è±¡

        Returns:
            éªŒè¯é€šè¿‡çš„Item

        Raises:
            DropItem: éªŒè¯å¤±è´¥
        """

        # éªŒè¯å¿…å¡«å­—æ®µ
        for field in self.required_fields:
            if not item.get(field):
                raise DropItem(
                    f'âŒ ç¼ºå°‘å¿…å¡«å­—æ®µ: {field}, URL: {item.get("url", "unknown")}'
                )

        # éªŒè¯æ ‡é¢˜é•¿åº¦
        title = item.get("title", "")
        if len(title) < self.MIN_TITLE_LENGTH:
            raise DropItem(
                f"âŒ æ ‡é¢˜è¿‡çŸ­ ({len(title)} < {self.MIN_TITLE_LENGTH}): {title}"
            )

        # éªŒè¯å†…å®¹é•¿åº¦
        content = item.get("content", "")
        if isinstance(content, list):
            content = " ".join(content)
        if content and len(content) < self.MIN_CONTENT_LENGTH:
            spider.logger.warning(
                f'âš  å†…å®¹è¾ƒçŸ­ ({len(content)} < {self.MIN_CONTENT_LENGTH}): {item.get("url")}'
            )

        # éªŒè¯URLæ ¼å¼
        url = item.get("url", "")
        if not url.startswith(("http://", "https://")):
            raise DropItem(f"âŒ æ— æ•ˆçš„URLæ ¼å¼: {url}")

        self.logger.debug(f"âœ… æ•°æ®å®Œæ•´æ€§éªŒè¯é€šè¿‡: {title[:50]}...")
        return item


class DeduplicationPipeline:
    """
    å»é‡ç®¡é“
    åŸºäºURLå’Œæ–°é—»IDè¿›è¡Œå†…å­˜çº§å»é‡
    """

    def __init__(self):
        self.seen_ids = set()
        self.seen_urls = set()
        self.logger = logging.getLogger(self.__class__.__name__)
        self.duplicate_count = 0

    def process_item(self, item, spider):
        """
        æ£€æŸ¥é‡å¤

        Args:
            item: NewsItemå¯¹è±¡
            spider: Spiderå¯¹è±¡

        Returns:
            å”¯ä¸€çš„Item

        Raises:
            DropItem: å‘ç°é‡å¤
        """

        # URL å»é‡
        url = item.get("url", "")
        if url in self.seen_urls:
            self.duplicate_count += 1
            raise DropItem(f"ğŸ”„ é‡å¤URL: {url}")
        self.seen_urls.add(url)

        # IDå»é‡
        news_id = item.get("news_id", "")
        if news_id in self.seen_ids:
            self.duplicate_count += 1
            raise DropItem(f"ğŸ”„ é‡å¤æ–°é—»ID: {news_id}")
        self.seen_ids.add(news_id)

        self.logger.debug(f'âœ… æ–°å†…å®¹: {item.get("title", "")[:50]}...')
        return item

    def close_spider(self, spider):
        """
        çˆ¬è™«å…³é—­æ—¶è¾“å‡ºå»é‡ç»Ÿè®¡

        Args:
            spider: Spiderå¯¹è±¡
        """
        self.logger.info(f"å»é‡ç»Ÿè®¡: å‘ç° {self.duplicate_count} æ¡é‡å¤æ•°æ®")


class DataCleaningPipeline:
    """
    æ•°æ®æ¸…æ´—ç®¡é“
    æ ‡å‡†åŒ–å’Œæ¸…æ´—å„ç±»æ•°æ®
    """

    def __init__(self):
        self.logger = logging.getLogger(self.__class__.__name__)

    def process_item(self, item, spider):
        """
        æ¸…æ´—æ•°æ®

        Args:
            item: NewsItemå¯¹è±¡
            spider: Spiderå¯¹è±¡

        Returns:
            æ¸…æ´—åçš„Item
        """
        # æ¸…æ´—æ ‡é¢˜
        if item.get("title"):
            item["title"] = self._clean_title(item["title"])

        # æ¸…æ´—å†…å®¹
        if item.get("content"):
            item["content"] = self._clean_content(item["content"])

        # æ¸…æ´—æ‘˜è¦
        if item.get("summary"):
            item["summary"] = self._clean_text(item["summary"])

        # æ ‡å‡†åŒ–æ—¶é—´æ ¼å¼
        for time_field in ["publish_time", "update_time", "crawl_time"]:
            if item.get(time_field):
                item[time_field] = self._standardize_time(item[time_field])

        # æ¸…æ´—URL
        if item.get("url"):
            item["url"] = self._clean_url(item["url"])

        # æ¸…æ´—å›¾ç‰‡åˆ—è¡¨
        if item.get("images"):
            item["images"] = self._clean_image_list(item["images"])

        # æ¸…æ´—æ ‡ç­¾åˆ—è¡¨
        if item.get("tags"):
            item["tags"] = self._clean_tags(item["tags"])

        return item

    def _clean_title(self, title: str) -> str:
        """
        æ¸…æ´—æ ‡é¢˜

        Args:
            title: æ ‡é¢˜å­—ç¬¦ä¸²

        Returns:
            æ¸…æ´—åçš„æ ‡é¢˜å­—ç¬¦ä¸²
        """
        if not title:
            return ""
        # ç§»é™¤å¤šä½™ç©ºæ ¼
        title = " ".join(title.split())
        # ç§»é™¤ç‰¹æ®Šå­—ç¬¦
        title = title.strip("\n\r\t")
        # ç§»é™¤å¸¸è§çš„æ ‡é¢˜åç¼€ï¼ˆå¦‚ç½‘ç«™åï¼‰
        for suffix in [" - CNN", " - BBC News", " | Reuters"]:
            if title.endswith(suffix):
                title = title[: -len(suffix)].strip()
        return title

    def _clean_content(self, content) -> str:
        """
        æ¸…æ´—æ­£æ–‡

        Args:
            content: æ­£æ–‡å­—ç¬¦ä¸²æˆ–åˆ—è¡¨

        Returns:
            æ¸…æ´—åçš„æ­£æ–‡å­—ç¬¦ä¸²
        """
        if isinstance(content, list):
            content = "\n".join(content)
        if not content:
            return ""
        # ç§»é™¤å¤šä½™ç©ºç™½ï¼Œä¿ç•™æ®µè½ç»“æ„
        lines = content.split("\n")
        cleaned_lines = [line.strip() for line in lines if line.strip()]
        return "\n".join(cleaned_lines)

    def _clean_text(self, text: str) -> str:
        """
        æ¸…æ´—æ–‡ç« æ‘˜è¦æ–‡æœ¬

        Args:
            text: æ–‡ç« æ‘˜è¦æ–‡æœ¬

        Returns:
            æ¸…æ´—åçš„æ–‡ç« æ‘˜è¦æ–‡æœ¬
        """
        if not text:
            return ""
        return " ".join(text.split()).strip()

    def _standardize_time(self, time_str) -> str:
        """
        æ ‡å‡†åŒ–æ—¶é—´æ ¼å¼ä¸ºISO 8601

        Args:
            time_str: æ—¶é—´å­—ç¬¦ä¸²

        Returns:
            æ ‡å‡†æ—¶é—´å­—ç¬¦ä¸²
        """
        if isinstance(time_str, datetime):
            return time_str.isoformat()

        # å¦‚æœå·²ç»æ˜¯ISOæ ¼å¼ï¼Œç›´æ¥è¿”å›
        if isinstance(time_str, str):
            return time_str.strip()
        return str(time_str)

    def _clean_url(self, url: str) -> str:
        """
        æ¸…æ´—URLï¼Œç§»é™¤è¿½è¸ªå‚æ•°

        Args:
            url: URLå­—ç¬¦ä¸²

        Returns:
            æ¸…æ´—åçš„URLå­—ç¬¦ä¸²
        """
        if not url:
            return ""
        url = url.strip()

        if "?" in url:
            url = url.split("?")[0]

        return url

    def _clean_image_list(self, images) -> list:
        """
        æ¸…æ´—å›¾ç‰‡URLåˆ—è¡¨

        Args:
            images: å›¾ç‰‡URLåˆ—è¡¨

        Returns:
            æ¸…æ´—åçš„å›¾ç‰‡URLåˆ—è¡¨
        """
        if not images:
            return []

        if isinstance(images, str):
            images = [images]

        cleaned = []
        for img_url in images:
            img_url = img_url.strip()
            # ç¡®ä¿æ˜¯æœ‰æ•ˆçš„URL
            if img_url.startswith(("http://", "https://", "//")):
                # å¤„ç†åè®®ç›¸å¯¹URL
                if img_url.startswith("//"):
                    img_url = "https:" + img_url
                cleaned.append(img_url)

        return cleaned

    def _clean_tags(self, tags) -> list:
        """
        æ¸…æ´—æ ‡ç­¾åˆ—è¡¨

        Args:
            tags: æ ‡ç­¾åˆ—è¡¨

        Returns:
            æ¸…æ´—åçš„æ ‡ç­¾åˆ—è¡¨
        """
        if not tags:
            return []

        if isinstance(tags, str):
            tags = [tags]

        # æ¸…æ´—å¹¶å»é‡
        cleaned = []
        seen = set()
        for tag in tags:
            tag = tag.strip().lower()
            if tag and tag not in seen:
                seen.add(tag)
                cleaned.append(tag)

        return cleaned


class MongoDBPipeline:
    """
    MongoDBå­˜å‚¨ç®¡é“
    å°†æ•°æ®å­˜å‚¨åˆ°MongoDBæ•°æ®åº“
    """

    def __init__(self, mongo_uri, mongo_db, collection_name):
        self.mongo_uri = mongo_uri
        self.mongo_db = mongo_db
        self.collection_name = collection_name
        self.client = None
        self.db = None
        self.collection = None
        self.logger = logging.getLogger(self.__class__.__name__)
        self.saved_count = 0
        self.updated_count = 0

    @classmethod
    def from_crawler(cls, crawler):
        """
        ä»Scrapy settingsä¸­è¯»å–é…ç½®

        Args:
            crawler: Scrapyçˆ¬è™«å¯¹è±¡

        Returns:
            MongoDBPipelineå®ä¾‹
        """
        return cls(
            mongo_uri=crawler.settings.get("MONGO_URI", "mongodb://localhost:27017"),
            mongo_db=crawler.settings.get("MONGO_DATABASE", "news_scraper"),
            collection_name=crawler.settings.get("MONGO_COLLECTION", "news"),
        )

    def open_spider(self, spider):
        """
        çˆ¬è™«å¼€å¯æ—¶è¿æ¥æ•°æ®åº“

        Args:
            spider: Scrapyçˆ¬è™«å¯¹è±¡
        """
        try:
            import pymongo

            self.client = pymongo.MongoClient(self.mongo_uri)
            self.db = self.client[self.mongo_db]
            self.collection = self.db[self.collection_name]

            # åˆ›å»ºç´¢å¼•
            self.collection.create_index("url", unique=True)
            self.collection.create_index("news_id", unique=True)
            self.collection.create_index("publish_time")
            self.collection.create_index("source_name")
            self.collection.create_index("category")
            self.collection.create_index([("source_name", 1), ("publish_time", -1)])

            spider.logger.info(
                f"âœ… MongoDBè¿æ¥æˆåŠŸ: {self.mongo_db}.{self.collection_name}"
            )
        except ImportError:
            spider.logger.error("âŒ pymongoæœªå®‰è£…ï¼ŒMongoDB Pipelineå·²ç¦ç”¨")
            spider.logger.error("   å®‰è£…å‘½ä»¤: pip install pymongo")
            raise
        except Exception as e:
            spider.logger.error(f"âŒ MongoDBè¿æ¥å¤±è´¥: {e}")
            raise

    def close_spider(self, spider):
        """
        çˆ¬è™«å…³é—­æ—¶æ–­å¼€è¿æ¥

        Args:
            spider: Scrapyçˆ¬è™«å¯¹è±¡
        """
        if self.client:
            self.client.close()
            spider.logger.info("âœ… MongoDBè¿æ¥å·²å…³é—­")
            spider.logger.info(
                f"å­˜å‚¨ç»Ÿè®¡: æ–°å¢ {self.saved_count} æ¡, æ›´æ–° {self.updated_count} æ¡"
            )

    def process_item(self, item, spider):
        """
        å­˜å‚¨æ•°æ®åˆ°MongoDB

        Args:
            item: Scrapy Itemå¯¹è±¡
            spider: Scrapyçˆ¬è™«å¯¹è±¡
        """
        try:
            # è½¬æ¢ä¸ºå­—å…¸
            data = dict(item)

            # ä½¿ç”¨upserté¿å…é‡å¤
            result = self.collection.update_one(
                {"url": data["url"]}, {"$set": data}, upsert=True
            )

            if result.upserted_id:
                self.saved_count += 1
                spider.logger.debug(f'ğŸ’¾ æ–°å¢: {data.get("title", "")[:50]}...')
            else:
                self.updated_count += 1
                spider.logger.debug(f'ğŸ”„ æ›´æ–°: {data.get("title", "")[:50]}...')

        except Exception as e:
            spider.logger.error(f"âŒ MongoDBå­˜å‚¨å¤±è´¥: {e}")
            spider.logger.error(f'   URL: {item.get("url")}')
            raise DropItem(f'å­˜å‚¨å¤±è´¥: {item.get("url")}')

        return item
